{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = make_classification(n_samples=1000, n_classes=3, n_informative=10\n",
    "                                       , n_features=15, class_sep=2.0)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    # The node for the ID3 decision tree\n",
    "    class TreeNode:\n",
    "        def __init__(self, attrId = -1, matchesLevel = [], children = [], returnVal = None):\n",
    "            # The index of the column to split out\n",
    "            self.attrId = attrId\n",
    "            \n",
    "            # List of one parameter lambda functions that return True/False \n",
    "            # based on whether the parameter matches the branch\n",
    "            self.matchesLevel = matchesLevel\n",
    "            \n",
    "            # List of child TreeNodes\n",
    "            self.children = children\n",
    "            \n",
    "            # The return value for this node, the class label if it's a leaf node, None if it is not\n",
    "            self.returnVal = returnVal\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.rootNode = None\n",
    "    \n",
    "    # Determines the best attribute to split by given features X and labels y\n",
    "    # Currently, this function assumes that X is numeric, and therefore computes\n",
    "    # split points by taking the average of each pair of ordered unique values\n",
    "    # Returns the column index of the best attribute and a list of functions\n",
    "    # that return True/False based on whether the attribute fits the split\n",
    "    def determineBestAttribute(X, y):\n",
    "        \n",
    "        # Computes the information in the labels\n",
    "        def computeInfo(y):\n",
    "            yVals = np.unique(y)\n",
    "            infoComps = np.zeros(len(yVals))\n",
    "            total = len(y)\n",
    "            for i in range(len(yVals)):\n",
    "                count = len(y[y == yVals[i]])\n",
    "                component = -(count / total) * np.log2(count / total)\n",
    "                infoComps[i] = component\n",
    "            return infoComps.sum()\n",
    "            \n",
    "        # array of information gain/other metric for each column, initialized to zeros\n",
    "        metric = np.zeros(X.shape[1])\n",
    "        \n",
    "        # list of list of functions that return True/False based on level of attribute, initially empty\n",
    "        matchFuncs = []\n",
    "        \n",
    "        # Compute best information and match functions for each column\n",
    "        for i in range(X.shape[1]):\n",
    "            \n",
    "            # check if attibute is categorical\n",
    "            if not np.issubdtype(X.dtype, np.number):\n",
    "                # Find all unique values and create functions to split on each value\n",
    "                vals = np.unique(X[:, i])\n",
    "                splitFuncs = [lambda x, v=v: x == v for v in vals]\n",
    "                \n",
    "                # Split the labels by each function\n",
    "                ySplits = [y[f(X[:, i])] for f in splitFuncs]\n",
    "                \n",
    "                # Compute the information gain for each split\n",
    "                infos = [computeInfo(ySplit) for ySplit in ySplits]\n",
    "                \n",
    "                # Take a weighted sum to get the total metric for this column\n",
    "                metric[i] = np.dot([len(split) for split in ySplits], infos) / len(y)\n",
    "                matchFuncs.append(splitFuncs)\n",
    "                continue\n",
    "\n",
    "            # create split points for the column\n",
    "            uniqueVals = np.unique(X[:, i])\n",
    "            splitPoints = (uniqueVals[:-1] + uniqueVals[1:]) / 2\n",
    "            \n",
    "            # Iterate through the split points, find the best one for this specific column\n",
    "            bestMetric = None\n",
    "            splitFuncs = None\n",
    "            \n",
    "            for sp in splitPoints:\n",
    "                matchFunc1 = lambda x, sp=sp: x < sp\n",
    "                matchFunc2 = lambda x, sp=sp: x >= sp\n",
    "                y1 = y[matchFunc1(X[:, i])]\n",
    "                y2 = y[matchFunc2(X[:, i])]\n",
    "                info1 = computeInfo(y1)\n",
    "                info2 = computeInfo(y2)\n",
    "                m = len(y1) / len(y) * info1 + len(y2) / len(y) * info2\n",
    "                if bestMetric == None or m < bestMetric:\n",
    "                    splitFuncs = [matchFunc1, matchFunc2]\n",
    "                    bestMetric = m\n",
    "\n",
    "            # save the best metric (ie information gain) for the column and the match function for it\n",
    "            metric[i] = bestMetric\n",
    "            matchFuncs.append(splitFuncs)\n",
    "\n",
    "        # Find the best attribute/match function from all the columns\n",
    "        bestAttr = np.argmin(metric)\n",
    "        return bestAttr, matchFuncs[bestAttr]\n",
    "    \n",
    "    # Creates a node in the tree with feature set X and labels y\n",
    "    def createNode(X, y):\n",
    "        # If there is only 1 unique label, predict that label\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return ID3.TreeNode(returnVal = y[0])\n",
    "        \n",
    "        # No features left, return the most abundant class in y\n",
    "        if X.shape[1] == 0:\n",
    "            return ID3.TreeNode(returnVal = mode(y).mode[0])\n",
    "        \n",
    "        # Determine the best attribute to split the dataset by and how to split it\n",
    "        bestAttr, matchesLevel = ID3.determineBestAttribute(X, y)\n",
    "        \n",
    "        # Create the children nodes with subsets of the features/labels\n",
    "        children = []\n",
    "        for i in range(len(matchesLevel)):\n",
    "            children.append(\n",
    "                ID3.createNode( \n",
    "                    X[matchesLevel[i](X[:, bestAttr]), :], y[matchesLevel[i](X[:, bestAttr])]\n",
    "                )\n",
    "            )\n",
    "        return ID3.TreeNode(attrId = bestAttr, matchesLevel = matchesLevel, children = children)\n",
    "\n",
    "    # Fits the ID3 decision tree with feature set X and labels y\n",
    "    def fit(self, X, y):\n",
    "        self.rootNode = ID3.createNode(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.rootNode == None:\n",
    "            return None\n",
    "        # Given a row from the feature set, predict the label\n",
    "        def predictTuple(x, node = self.rootNode):\n",
    "            if node.returnVal != None:\n",
    "                return node.returnVal\n",
    "            for i in range(len(node.matchesLevel)):\n",
    "                f = node.matchesLevel[i]\n",
    "                if f(x[node.attrId]):\n",
    "                    return predictTuple(x, node.children[i])\n",
    "            # Attribute has a new value not seen before by the tree, returning None for the label\n",
    "            return None\n",
    "        \n",
    "        # Iterate through all the rows, predict the label for each\n",
    "        preds = [None for v in range(X.shape[0])]\n",
    "        for i in range(X.shape[0]):\n",
    "            preds[i] = predictTuple(X[i, :])\n",
    "        return preds\n",
    "    \n",
    "    # For debugging\n",
    "    def printTreeBreadth(self):\n",
    "        strStack = []\n",
    "        nodeStack = [(0, self.rootNode)]\n",
    "        while len(nodeStack) != 0:\n",
    "            level, n = nodeStack.pop(0)\n",
    "            for i in range(level):\n",
    "                print(\"  \", end=\"\")\n",
    "            if n.returnVal != None:\n",
    "                print(\"Return val of\", n.returnVal)\n",
    "                continue\n",
    "            print(\"Attribute\", n.attrId)\n",
    "            for c in n.children:\n",
    "                nodeStack.append((level + 1, c))\n",
    "        \n",
    "    def printTreeDepth(self):\n",
    "        def printNode(n = self.rootNode, level = 0):\n",
    "            for i in range(level):\n",
    "                print(\"  \", end = \"\")\n",
    "            if n.returnVal != None:\n",
    "                print(\"Return val of\", n.returnVal)\n",
    "                return\n",
    "            print(\"Attribute\", n.attrId)\n",
    "            for c in n.children:\n",
    "                printNode(c, level + 1)\n",
    "        printNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.81      0.84        85\n",
      "          1       0.82      0.89      0.85        87\n",
      "          2       0.91      0.90      0.90        78\n",
      "\n",
      "avg / total       0.87      0.86      0.86       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id3 = ID3()\n",
    "id3.fit(features_train, labels_train)\n",
    "predictions = id3.predict(features_test)\n",
    "print(classification_report(labels_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.81      0.84        85\n",
      "          1       0.81      0.89      0.85        87\n",
      "          2       0.92      0.90      0.91        78\n",
      "\n",
      "avg / total       0.87      0.86      0.86       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtc = dtc.fit(features_train, labels_train)\n",
    "dtc_prediction = dtc.predict(features_test)\n",
    "\n",
    "print(classification_report(labels_test, dtc_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        79\n",
      "          1       0.94      0.93      0.93        95\n",
      "          2       0.95      0.96      0.95        76\n",
      "\n",
      "avg / total       0.94      0.94      0.94       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dtc_prediction, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       1.00      1.00      1.00         4\n",
      "        yes       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_example = np.array(pd.read_csv(\"naive_bayes/example.csv\"))\n",
    "features = cat_example[:, :-1]\n",
    "labels = cat_example[:, -1]\n",
    "cat_id3 = ID3()\n",
    "cat_id3.fit(features, labels)\n",
    "cat_preds = cat_id3.predict(features)\n",
    "print(classification_report(cat_preds, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
